{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for start training neural nets on my LFP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since noteboke doesn't work in jupiterlabs    %matplotlib notebook \n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import sys\n",
    "import scipy.io as sio\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "import ipywidgets\n",
    "import PIL\n",
    "#import itertools\n",
    "import os\n",
    "from numpy.lib.format import open_memmap\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import net layers structure and optimizers from Keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Concatenate\n",
    "from keras.layers import Convolution2D, MaxPooling2D,Conv2D\n",
    "from keras.layers.core import Lambda\n",
    "import keras.optimizers as optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlignedLFP(cellType,cre = None, mice = None, period = None, day=None, drug=None,drugPeriod='Pre'):\n",
    "    # function that take in the classification and return the appropreate data:\n",
    "    #Inputs:\n",
    "    #   cellType - return MSN or CRE if both pass ['MNS','CRE']\n",
    "    #   mice - (Optional) list of mice from to include. Default: None - will load data for all mice\n",
    "    #   period - (Optional) either 'Pre' or 'Post'. difault: None - return full length of data from picked sessions\n",
    "    #   day - (Optional) lambda function with logic for picking days. Default: None - ignore day attr when picking data\n",
    "    #           NOTE: day will be ignored if period is specified\n",
    "    #   cre - (Optional) which cre mouse is it. options:None (default), \"PV\", \"CHI\"\n",
    "    #                   must have trace included in dataType list to be taken into account\n",
    "    #   WinPre - (Optional) length of pre window in secounds (default 2)\n",
    "    #   WinPost - (Optional) length of post window in secounds (default 2)\n",
    "    #Output:\n",
    "    #   data - the requested data. format: {mice_session:{dataType:data}}\n",
    "    \n",
    "    \n",
    "    dFile = 'FinalData_6OHDA_H.h5'\n",
    "    # double check parameters inputs are valid:\n",
    "    if drugPeriod=='Post':\n",
    "        savePath = '/home/dana_z/HD1/lfp2ca_notNormalize/'#'/home/dana_z/HD1/lfpAligned2Ca/Post/'\n",
    "    else:\n",
    "        savePath = '/home/dana_z/HD1/lfp2ca_notNormalize/'#'/home/dana_z/HD1/lfpAligned2Ca/Pre/'\n",
    "\n",
    "    df = pd.read_csv(savePath+'sessions')\n",
    "    \n",
    "    if period == None and day != None and isinstance(day,type(lambda c:None)):\n",
    "        df['keep'] = df.apply(lambda row: day(row.day), axis=1)\n",
    "        df = df[(df.keep==True)]\n",
    "    \n",
    "    if period in ['Healthy','Day 1-4','Day 5-12','Day 13-20','One Month']:\n",
    "        df = df[(df.period==period)]\n",
    "       \n",
    "    if cre in ['PV','CHI','NA']:\n",
    "        df = df[(df.cre==cre)]\n",
    "    \n",
    "    if drug in ['Amph','L-Dopa','Saline','None']:\n",
    "        df = df[(df.drug==drug)]\n",
    "    \n",
    "\n",
    "    if not isinstance(cellType,list):\n",
    "        cellType = [cellType]\n",
    "        \n",
    "    cellType = list(set(cellType).intersection(set(['MSN','CRE'])))\n",
    "    if len(cellType) == 0:\n",
    "        raise ValueError('Not a valid cellType value. cellType must be in [\"MSN\",\"CRE\"]')\n",
    "    \n",
    "    # traverse the hdf5 file:\n",
    "    if mice == None:\n",
    "        mice = getMiceList(dFile) \n",
    "    elif not isinstance(mice,list):\n",
    "        mice = [mice]\n",
    "    \n",
    "    if not isinstance(mice[0],str):\n",
    "        for m in range(0,len(mice)):\n",
    "            mice[m] = str(mice[m])\n",
    "    df = df[(df.mouse.isin(mice))]\n",
    "    # start extracting the data:   \n",
    "    \n",
    "    # alllocate memory:\n",
    "    nNeurons = 0;\n",
    "    if 'MSN' in cellType:\n",
    "        nNeurons = nNeurons + int(df.numMsn.sum()) - int(df.numred.sum())\n",
    "    if 'CRE' in cellType:\n",
    "        nNeurons = nNeurons + int(df.numred.sum())\n",
    "    \n",
    "    dResult = np.empty([12206,87,nNeurons],dtype=float)\n",
    "    \n",
    "    ind = 0\n",
    "    for sess in df.sess.unique():\n",
    "        if 'MSN' in cellType:\n",
    "            tempD = pickle.load(open(savePath+'MSN/'+sess,'rb'))\n",
    "            tempD[tempD==9999] = 0\n",
    "            tempD[tempD==-9999] = 0\n",
    "            dResult[:,:,ind:ind+tempD.shape[2]] = tempD   \n",
    "            ind = ind+tempD.shape[2]\n",
    "        # for every Cre neuron:\n",
    "        if 'CRE' in cellType:\n",
    "            try:\n",
    "                tempD = pickle.load(open(savePath+'CRE/'+sess,'rb'))\n",
    "            except:\n",
    "                continue\n",
    "            tempD[tempD==9999] = 0\n",
    "            tempD[tempD==-9999] = 0\n",
    "            dResult[:,:,ind:ind+tempD.shape[2]] = tempD   \n",
    "            ind = ind+tempD.shape[2]\n",
    "        \n",
    "    return dResult[:,:,:ind],df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dana_z/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/dana_z/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12206, 87, 1388) (1388,)\n",
      "{'0': 'Healthy', '1': 'Day 5-12', '2': 'Day 13-20', '3': 'Day 1-4', '4': 'One Month'}\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# figure out proper in out function later -> just for debugging\n",
    "# dataset_folder = ['/home/dana_z/HD1/lfp2ca_notNormalize/']\n",
    "# dataF = '/home/dana_z/HD1/CNN_data/'\n",
    "# mice = 'All'\n",
    "\n",
    "df = pd.read_csv(dataset_folder[0] + \"sessions\")\n",
    "df.groupby(['period','cre']).sum()\n",
    "TrainMouse = '8803'\n",
    "categoryDict = {}\n",
    "for indP,per in enumerate(df.period.unique()):\n",
    "    A,df2 = getAlignedLFP('MSN', period = per,mice=TrainMouse)\n",
    "    A *= 1.00/np.amax(A,axis=(0,1))\n",
    "    if 'trainData' not in locals():\n",
    "        trainData = A\n",
    "        trainLabels = np.zeros((A.shape[2],))+indP\n",
    "    else:\n",
    "        trainData = np.concatenate((trainData,A),axis=2)\n",
    "        trainLabels = np.concatenate((trainLabels,np.zeros((A.shape[2],))+indP))\n",
    "    categoryDict[str(indP)] = per;\n",
    "a = np.argwhere(np.isnan(np.amax(trainData,axis=(0,1))))\n",
    "trainData = trainData[:,:,[i for i in range(trainData.shape[2]) if i not in a]]\n",
    "trainLabels = trainLabels[[i for i in range(len(trainLabels)) if i not in a]]\n",
    "print(trainData.shape,trainLabels.shape)\n",
    "print(categoryDict)\n",
    "print(np.amax(trainData,axis=(0,1)))\n",
    "trainData = np.expand_dims(trainData,2)                          \n",
    "trainData = np.moveaxis(trainData,-1,0)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1388, 12206, 87, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainData = np.expand_dims(trainData,2)\n",
    "\n",
    "trainData2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(10, (3, 3), input_shape=(12206,87,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(20, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(10, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5))\n",
    "\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 122204, 85, 64)    640       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 122204, 85, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 61102, 42, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 61100, 40, 128)    73856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 61100, 40, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30550, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30548, 18, 64)     73792     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 30548, 18, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15274, 9, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8797824)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 43989125  \n",
      "=================================================================\n",
      "Total params: 44,137,413\n",
      "Trainable params: 44,137,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(trainData, trainLabels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8430_BaselineS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Files:  50%|█████     | 1/2 [00:04<00:04,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12206, 87, 64)\n",
      "8803_day19L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files: 100%|██████████| 2/2 [00:11<00:00,  5.89s/it]\n",
      "Files:  50%|█████     | 1/2 [00:00<00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12206, 87, 104)\n",
      "4539_day10\n",
      "(12206, 87, 2)\n",
      "1236_BaselineS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files: 100%|██████████| 2/2 [00:00<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12206, 87, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def periodCalc(sess):\n",
    "    \n",
    "    if sess[5] == 'B':\n",
    "            day = 0\n",
    "        else:\n",
    "            day = int(re.findall(r'\\d+',sess[5:])[0])\n",
    "    \n",
    "    if day== 0:\n",
    "        return 'Healthy'\n",
    "    elif day<5:\n",
    "        return 'Day 1-4'\n",
    "    elif day<13:\n",
    "        return 'Day 5-12'\n",
    "    elif day<21:\n",
    "        return 'Day 13-20'\n",
    "    else:\n",
    "        return 'One Month'\n",
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_list:\n",
    "    dataset_folder = os.path.normpath(dataset_folder)\n",
    "\n",
    "    # Get image file names from folder\n",
    "    image_filename_list = next(os.walk(dataset_folder))[2]\n",
    "    for current_image_filename in tqdm(image_filename_list[1:3], desc='Files'): #remove the [1] after debugging\n",
    "        print(current_image_filename,periodCalc(current_image_filename))\n",
    "        filePeriod = periodCalc(current_image_filename)\n",
    "        current_image_path = os.path.join(dataset_folder, current_image_filename)\n",
    "        # load data\n",
    "        tempD = pickle.load(open(current_image_path,'rb'))\n",
    "        tempD[tempD==9999] = np.nan\n",
    "        tempD[tempD==-9999] = np.nan\n",
    "        print(tempD.shape)\n",
    "#         self.image_file_list.append(current_image_path)\n",
    "        # calc how many cells\n",
    "        if image_stack.series[0].ndim==3:\n",
    "            image_width, image_heigh ,numCells  = image_stack.series[0].shape\n",
    "        elif image_stack.series[0].ndim==2:\n",
    "            numCells = 1\n",
    "            image_width, image_heigh  = image_stack.series[0].shape\n",
    "        else:\n",
    "            print(\"Skip {}\".format(current_image_filename))\n",
    "            break\n",
    "        for cell in range(0,numCells):\n",
    "            data_info = {\n",
    "                            \"id\": current_image_filename+\"_\"+str(cell),\n",
    "                            \"path\": current_image_path,\n",
    "                            \"idx\": cell,\n",
    "                            \"period\": filePeriod\n",
    "                        }\n",
    "            self.data_info.append(data_info)\n",
    "            \n",
    "            freq_idx_max = trace_cfs.shape[0] \n",
    "        freq_idx_min = 0\n",
    "        print(\"Frequency range: {:1.2f}-{:1.2f} Hz (max idx: {:1.2f} ).\".format(trace_cfs_freq[freq_idx_max-1],trace_cfs_freq[freq_idx_min],freq_idx_max))\n",
    "        try:                   \n",
    "            if len(self.frequency_idx)==2:\n",
    "                freq_idx_min = np.amax([freq_idx_min,self.frequency_idx[0]])\n",
    "                freq_idx_max = np.amin([freq_idx_max,self.frequency_idx[1]+1])\n",
    "            elif len(self.frequency_idx)==1:\n",
    "                freq_idx_min = np.amax([freq_idx_min,freq_idx_max-self.frequency_idx[0]])\n",
    "        except:\n",
    "            pass\n",
    "        self.frequency_idx = [freq_idx_min,freq_idx_max]\n",
    "        self.frequency_idx_length = self.frequency_idx[1]-self.frequency_idx[0]\n",
    "        print(\"Selected frequency range: {:1.2f}-{:1.2f} Hz ({:1.2f} data points).\".format(trace_cfs_freq[self.frequency_idx[1]-1],trace_cfs_freq[self.frequency_idx[0]],self.frequency_idx_length))\n",
    "        print(\"Total {} samples.\".format(len(self.data_info)))\n",
    "\n",
    "        # calculate sample with events\n",
    "\n",
    "        event_data_id_list = []\n",
    "        non_event_data_id_list = []\n",
    "        for data_id in tqdm(range(len(self.data_info)),desc=\"Calculating sample with events\"):\n",
    "            if self.data_info[data_id]['event']==1:\n",
    "                event_data_id_list.append(data_id)\n",
    "            elif self.data_info[data_id]['event']==0:\n",
    "                non_event_data_id_list.append(data_id)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        print(\"Sample with event: {} ({}%)\".format(len(event_data_id_list),100*len(event_data_id_list)/len(self.data_info)))\n",
    "        print(\"Sample without event: {} ({}%)\".format(len(non_event_data_id_list),100*len(non_event_data_id_list)/len(self.data_info)))\n",
    "\n",
    "        self.event_data_id_list = event_data_id_list\n",
    "        self.non_event_data_id_list = non_event_data_id_list\n",
    "\n",
    "        total_id_count = np.ceil(np.amax((len(self.non_event_data_id_list)/(1-self.event_ratio),len(self.event_data_id_list)/self.event_ratio)))\n",
    "        event_data_id_list = np.repeat(self.event_data_id_list,int(np.ceil((self.event_ratio*total_id_count)/len(self.event_data_id_list))))\n",
    "        non_event_data_id_list = np.repeat(self.non_event_data_id_list,int(np.ceil(((1-self.event_ratio)*total_id_count)/len(self.non_event_data_id_list))))\n",
    "        np.random.shuffle(event_data_id_list)\n",
    "        np.random.shuffle(non_event_data_id_list)\n",
    "        event_data_id_list = event_data_id_list[:int(self.event_ratio*total_id_count)]\n",
    "        non_event_data_id_list = non_event_data_id_list[:int((1-self.event_ratio)*total_id_count)]\n",
    "\n",
    "        print(\"Adjusted sample with event: {} ({}%)\".format(len(event_data_id_list),100*len(event_data_id_list)/(len(event_data_id_list)+len(non_event_data_id_list))))\n",
    "        print(\"Adjusted sample without event: {} ({}%)\".format(len(non_event_data_id_list),100*len(non_event_data_id_list)/(len(event_data_id_list)+len(non_event_data_id_list))))\n",
    "\n",
    "        self.on_epoch_end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
